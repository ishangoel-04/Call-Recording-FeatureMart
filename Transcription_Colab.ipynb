{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Call recording transcription (faster-whisper)\n",
        "\n",
        "Run this notebook in Google Colab. It installs faster-whisper and ffmpeg, then transcribes one audio file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install faster-whisper -q\n",
        "!apt-get install -y ffmpeg -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and upload audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from faster_whisper import WhisperModel\n",
        "from google.colab import files\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# Upload one audio file (popup will appear)\n",
        "uploaded = files.upload()\n",
        "audio_path = list(uploaded.keys())[0]\n",
        "log.info(\"Using audio file: %s\", audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load model and transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log.info(\"Loading Whisper model (large-v3)...\")\n",
        "model = WhisperModel(\"large-v3\", device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "log.info(\"Starting transcription: %s\", audio_path)\n",
        "segments, info = model.transcribe(audio_path, beam_size=5)\n",
        "\n",
        "log.info(\"Language: %s (prob: %.2f)\", info.language, info.language_probability)\n",
        "log.info(\"Duration: %.2fs\", info.duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Segments and full transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_text = []\n",
        "for segment in segments:\n",
        "    print(f\"[{segment.start:.2f}s --> {segment.end:.2f}s] {segment.text}\")\n",
        "    full_text.append(segment.text)\n",
        "\n",
        "full_transcript = \" \".join(full_text).strip()\n",
        "log.info(\"Transcription complete. Segments: %d\", len(full_text))\n",
        "if full_transcript:\n",
        "    log.info(\"Full text length: %d chars\", len(full_transcript))\n",
        "\n",
        "print(\"\\n--- Full transcript ---\")\n",
        "print(full_transcript)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
